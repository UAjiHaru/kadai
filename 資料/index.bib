@misc{translate,
  title={単語を箱で表現！新たな埋め込み手法 {B}ox {E}mbedding を基礎から理解},
  howpublished={\url{https://ja.stateofaiguides.com/20221013-box-embeddings/}},
  author={萩原 正人},
  year={2022},
  month={10},
  note={(Accessed on 11/29/2022)}
}

@inproceedings{dasgupta-etal-2022-word2box,
    title = "{W}ord2{B}ox: Capturing Set-Theoretic Semantics of Words using Box Embeddings",
    author = "Dasgupta, Shib  and
      Boratko, Michael  and
      Mishra, Siddhartha  and
      Atmakuri, Shriya  and
      Patel, Dhruvesh  and
      Li, Xiang  and
      McCallum, Andrew",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.161",
    doi = "10.18653/v1/2022.acl-long.161",
    pages = "2263--2276",
    abstract = "Learning representations of words in a continuous space is perhaps the most fundamental task in NLP, however words interact in ways much richer than vector dot product similarity can provide. Many relationships between words can be expressed set-theoretically, for example, adjective-noun compounds (eg. {``}red cars{''}⊆{``}cars{''}) and homographs (eg. {``}tongue{''}∩{``}body{''} should be similar to {``}mouth{''}, while {``}tongue{''}∩{``}language{''} should be similar to {``}dialect{''}) have natural set-theoretic interpretations. Box embeddings are a novel region-based representation which provide the capability to perform these set-theoretic operations. In this work, we provide a fuzzy-set interpretation of box embeddings, and learn box representations of words using a set-theoretic training objective. We demonstrate improved performance on various word similarity tasks, particularly on less common words, and perform a quantitative and qualitative analysis exploring the additional unique expressivity provided by Word2Box.",
}